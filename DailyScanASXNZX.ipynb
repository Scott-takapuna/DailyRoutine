{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9749da-27c5-4e44-a78b-85cfd51e3c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing CHN.AX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing 1. CHN.AX with n_components=2, degree=2, std_multiplier=2.5, train_window=252\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing TPW.AX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing 2. TPW.AX with n_components=2, degree=3, std_multiplier=2.5, train_window=189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ No new signals in the last 2 days. No Signal Alert file created.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    119\u001b[39m     Stop_loss = \u001b[38;5;28mfloat\u001b[39m(row[\u001b[33m'\u001b[39m\u001b[33mMin_StopLoss\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔍 Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with n_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, degree=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegree\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, std_multiplier=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_multiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, train_window=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     process_ticker_to_excel(ticker, start_date, end_date, n_components, degree, std_multiplier, train_window, rank, sector, industry, title, trades, Strat_return, Stop_loss)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠️ Skipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m due to missing/invalid parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mprocess_ticker_to_excel\u001b[39m\u001b[34m(ticker, start_date, end_date, n_components, degree, std_multiplier, train_window, rank, sector, industry, title, trades, Strat_return, Stop_loss)\u001b[39m\n\u001b[32m     68\u001b[39m data = get_clean_financial_data(ticker, start_date, end_date)\n\u001b[32m     69\u001b[39m X, y, data, scaler = prepare_data(data)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m y_pred_scaled = train_models_rolling(X, y, n_components, degree, train_window)\n\u001b[32m     71\u001b[39m y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).flatten()\n\u001b[32m     72\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mPredicted\u001b[39m\u001b[33m'\u001b[39m] = y_pred\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_models_rolling\u001b[39m\u001b[34m(X, y, n_components, degree, train_window)\u001b[39m\n\u001b[32m     41\u001b[39m gmm = GaussianMixture(n_components=n_components, covariance_type=\u001b[33m'\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     42\u001b[39m gmm.fit(X_train)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m latent_features = gmm.predict_proba(X_train)\n\u001b[32m     44\u001b[39m X_latent = np.hstack([X_train, latent_features])\n\u001b[32m     45\u001b[39m poly_reg = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Scott1\\Lib\\site-packages\\sklearn\\mixture\\_base.py:402\u001b[39m, in \u001b[36mBaseMixture.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    400\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    401\u001b[39m X = validate_data(\u001b[38;5;28mself\u001b[39m, X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m _, log_resp = \u001b[38;5;28mself\u001b[39m._estimate_log_prob_resp(X)\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.exp(log_resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Scott1\\Lib\\site-packages\\sklearn\\mixture\\_base.py:525\u001b[39m, in \u001b[36mBaseMixture._estimate_log_prob_resp\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    507\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[32m    508\u001b[39m \n\u001b[32m    509\u001b[39m \u001b[33;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m \u001b[33;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     weighted_log_prob = \u001b[38;5;28mself\u001b[39m._estimate_weighted_log_prob(X)\n\u001b[32m    526\u001b[39m     log_prob_norm = logsumexp(weighted_log_prob, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(under=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    528\u001b[39m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Scott1\\Lib\\site-packages\\sklearn\\mixture\\_base.py:478\u001b[39m, in \u001b[36mBaseMixture._estimate_weighted_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    468\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[32m    469\u001b[39m \n\u001b[32m    470\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    476\u001b[39m \u001b[33;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[32m    477\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._estimate_log_prob(X) + \u001b[38;5;28mself\u001b[39m._estimate_log_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Scott1\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:820\u001b[39m, in \u001b[36mGaussianMixture._estimate_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_log_gaussian_prob(\n\u001b[32m    821\u001b[39m         X, \u001b[38;5;28mself\u001b[39m.means_, \u001b[38;5;28mself\u001b[39m.precisions_cholesky_, \u001b[38;5;28mself\u001b[39m.covariance_type\n\u001b[32m    822\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Scott1\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:480\u001b[39m, in \u001b[36m_estimate_log_gaussian_prob\u001b[39m\u001b[34m(X, means, precisions_chol, covariance_type)\u001b[39m\n\u001b[32m    478\u001b[39m     log_prob = np.empty((n_samples, n_components))\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n\u001b[32m    481\u001b[39m         log_prob[:, k] = np.sum(np.square(y), axis=\u001b[32m1\u001b[39m)\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m covariance_type == \u001b[33m\"\u001b[39m\u001b[33mtied\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import logging\n",
    "\n",
    "# === Configuration ===\n",
    "TICKER_FILE = 'C:\\\\Users\\\\User\\\\OneDrive\\\\Momentum\\\\Scripts\\\\Deep_Learn_Analysis\\\\Parameter_Library\\\\Params_MASTER.xlsx'\n",
    "years = 7\n",
    "signal_alerts = []\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(name)s:%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_clean_financial_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    data.columns = data.columns.get_level_values(0)\n",
    "    data = data.ffill()\n",
    "    data.index = data.index.tz_localize(None)\n",
    "    return data\n",
    "\n",
    "def prepare_data(data):\n",
    "    data = data.reset_index()\n",
    "    data['Date_Ordinal'] = pd.to_numeric(data['Date'].map(pd.Timestamp.toordinal))\n",
    "    scaler = StandardScaler()\n",
    "    data['Close_Scaled'] = scaler.fit_transform(data[['Close']])\n",
    "    X = data[['Date_Ordinal']].values\n",
    "    y = data['Close_Scaled'].values\n",
    "    return X, y, data, scaler\n",
    "\n",
    "def train_models_rolling(X, y, n_components, degree, train_window):\n",
    "    y_pred = np.full_like(y, np.nan)\n",
    "    for i in range(train_window, len(X)):\n",
    "        X_train = X[i-train_window:i]\n",
    "        y_train = y[i-train_window:i]\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
    "        gmm.fit(X_train)\n",
    "        latent_features = gmm.predict_proba(X_train)\n",
    "        X_latent = np.hstack([X_train, latent_features])\n",
    "        poly_reg = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "        poly_reg.fit(X_latent, y_train)\n",
    "        current_latent = gmm.predict_proba(X[i].reshape(1, -1))\n",
    "        current_X_latent = np.hstack([X[i].reshape(1, -1), current_latent])\n",
    "        y_pred[i] = poly_reg.predict(current_X_latent)[0]\n",
    "    return y_pred\n",
    "\n",
    "def generate_signals_rolling(data, y_pred, train_window, std_multiplier):\n",
    "    window = train_window\n",
    "    residuals = data['Close'] - y_pred\n",
    "    data['Upper_Bound'] = np.nan\n",
    "    data['Lower_Bound'] = np.nan\n",
    "    for i in range(window, len(data)):\n",
    "        current_std = np.std(residuals.iloc[i-window:i])\n",
    "        data.loc[data.index[i], 'Upper_Bound'] = y_pred[i] + std_multiplier * current_std\n",
    "        data.loc[data.index[i], 'Lower_Bound'] = y_pred[i] - std_multiplier * current_std\n",
    "    data['Buy_Signal'] = np.where(data['Close'] < data['Lower_Bound'], 1, 0)\n",
    "    data['Sell_Signal'] = np.where(data['Close'] > data['Upper_Bound'], 1, 0)\n",
    "    return data\n",
    "\n",
    "def process_ticker_to_excel(ticker, start_date, end_date, n_components, degree, std_multiplier, train_window, rank, sector, industry, title, trades, Strat_return, Stop_loss):\n",
    "    try:\n",
    "        logger.info(f\"Processing {ticker}\")\n",
    "        data = get_clean_financial_data(ticker, start_date, end_date)\n",
    "        X, y, data, scaler = prepare_data(data)\n",
    "        y_pred_scaled = train_models_rolling(X, y, n_components, degree, train_window)\n",
    "        y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        data['Predicted'] = y_pred\n",
    "        data = generate_signals_rolling(data, y_pred, train_window, std_multiplier)\n",
    "        # === Extract recent signals (preceding day) ===\n",
    "        recent_cutoff = pd.Timestamp.today() - pd.Timedelta(days=4)\n",
    "        recent_signals = data[(data['Date'] >= recent_cutoff) & ((data['Buy_Signal'] == 1) | (data['Sell_Signal'] == 1))]\n",
    "        for _, row in recent_signals.iterrows():\n",
    "            signal_type = 'BUY' if row['Buy_Signal'] == 1 else 'SELL'\n",
    "            signal_alerts.append({\n",
    "                'Ticker': ticker,\n",
    "                'Title': title,\n",
    "                'n_components': n_components,\n",
    "                'degree': degree,\n",
    "                'std_multiplier': std_multiplier,\n",
    "                'train_window': train_window,\n",
    "                'Signal_Type': signal_type,\n",
    "                'Return': Strat_return,\n",
    "                'Transactions': trades,\n",
    "                'Stop Loss': Stop_loss,\n",
    "                'Date': row['Date'].strftime('%Y-%m-%d'),\n",
    "                'Price': row['Close'],\n",
    "                'Rank' : rank,\n",
    "                'Industry' : industry,\n",
    "                'Sector' : sector,\n",
    "            })\n",
    " \n",
    "        return signal_alerts\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error processing {ticker}: {e}\")\n",
    "        \n",
    "def main():    \n",
    "    tickers_df = pd.read_excel(TICKER_FILE)\n",
    "    start_date = pd.Timestamp.today() - pd.DateOffset(years=years)\n",
    "    end_date = pd.Timestamp.today()\n",
    "    futures = []\n",
    "    for _, row in tickers_df.iterrows():\n",
    "        ticker = row['Ticker']\n",
    "        try:\n",
    "            n_components = int(row['n_components'])\n",
    "            degree = int(row['poly_degree'])\n",
    "            std_multiplier = float(row['std_multiplier'])\n",
    "            train_window = int(row['train_window'])\n",
    "            rank = int(row['Rank'])\n",
    "            sector = row['Sector']\n",
    "            industry = row['Industry']\n",
    "            title = row['Title']\n",
    "            trades = int(row['# of trades'])\n",
    "            Strat_return = float(row['Strategy Run Rate'])\n",
    "            Stop_loss = float(row['Min_StopLoss%'])\n",
    "\n",
    "            print(f\"🔍 Processing {rank}. {ticker} with n_components={n_components}, degree={degree}, std_multiplier={std_multiplier}, train_window={train_window}\")\n",
    "            process_ticker_to_excel(ticker, start_date, end_date, n_components, degree, std_multiplier, train_window, rank, sector, industry, title, trades, Strat_return, Stop_loss)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {ticker} due to missing/invalid parameters: {e}\")\n",
    "          \n",
    "    # === Write Signal Alert CSV with timestamp ===\n",
    "    if signal_alerts:\n",
    "        alerts_df = pd.DataFrame(signal_alerts)\n",
    "        alerts_df.sort_values(by='Date', ascending=False, inplace=True)\n",
    "\n",
    "        alert_filename = f'Signal_Archive/Signal_Alert_from_Market_Scanner.csv'\n",
    "        alert_log = f'Signal_Archive/Signal_Alert_log.csv'\n",
    "        \n",
    "        alerts_df.to_csv(alert_filename, index=False)\n",
    "        \n",
    "        if os.path.exists(alert_log):\n",
    "            # Append to the existing file without writing the header\n",
    "            alerts_df.to_csv(alert_log, mode='a', header=False, index=False)\n",
    "        else:\n",
    "        # If the file does not exist, create it and write the header\n",
    "            alerts_df.to_csv(alert_log, mode='w', header=True, index=False)\n",
    "\n",
    "        print(f\"\\n🚨 Signal Alert CSV created: {alert_filename} ({len(alerts_df)} recent signals found)\\n\")\n",
    "    else:\n",
    "        print(\"\\n✅ No new signals in the last 2 days. No Signal Alert file created.\\n\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dd2dc-5bfd-476f-88dd-40c0af0f9286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scott1",
   "language": "python",
   "name": "scott1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
